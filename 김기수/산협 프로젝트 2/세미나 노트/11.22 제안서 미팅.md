이슈 두 개
거대 언어 모델 운용(LLMOps)을 위한 GPU 클러스터 최적화 기법 연구
Ops 단어가 애매모호함.
Issue 1 - New keywords
langchain, RAG, AI Agent 세 가지 키워드
RAG도 한계가 있어서 이런 것을 포함해서 나온 것이 AI Agent임.
LangChain이란? 프롬프트를 정리하고 관리해주는 개념. 매개변수 조정, 프롬프트 보강, 응답 조정에 이르기 까지 LLM을 사용하기 위한 API 레이어임.
RAG는 개인적인 정보나 서비스와 관련한 한계가 있음. 이러한 한계를 극복하기 위해 AI Agent가 도입됨. 유저의 결정이나 행동을 대신할 수 있을 정도(* 치과 예약 등)

Issue 2 - 연간 계획
1. Intra-node
2. Inter-node
3. Cluster-wise

OSDI 논문을 보았음
serving 쪽에 가까움 성호,마이카님이 보고 있는 것. 할 수 있는 얘기가 더 많을 것으로 보임.
AI Agent는 애매함 -> app에 너무 가까움
RAG는 서빙에 넣기는 애매함 그러나 TCP/IP를 넣을 수 있어서 좋겠지만 더 조사를 해봐야할 듯
연속적인 리퀘스트를 서버리스 형태로 각각의 파인튜닝 된 모델을 불러와야 한다.
스피드 업이나 공정성에 대한 이슈도 말할 수 있을 것 같음.
AI Agent는 ..?
LLM service operator ...?

**RAG가 어떠한 시스템적인 이슈를 일으키는지 확인해 볼 것.**
**langchain도 확인해 볼 것.**

kv cache는 반드시 들어가야할 듯. 트랜스포머 모델도 알아야 함.

